# 人工智能隐私保护
## 数据隐私风险揭秘
### 人工智能隐私泄露案例
主流大语言模型集体曝出训练数据泄露漏洞  
案例：让ChatGPT重复同一个词  
现有的大语言模型安全措施：模型对齐、内容记忆测试  

如何让ai“嘴漏”  

### 主流的隐私泄露攻击方法  

* 成员推断攻击
    * 攻击者目的：攻击者试图判断某个目标输入样本是否存在于目标模型的训练数据集中
    * 攻击者能力：黑盒攻击设置，攻击者只能够访问模型，输入样本可以得到输出结果
    * 攻击原理：模型在训练集和非训练集上的输出结果往往不同
    * 攻击模型是二分类器
* 模型反演攻击
    * 攻击者目的：试图通过对模型的查询或暴露其部分内部信息来恢复训练数据
    * 攻击者分类
        * 白盒模型反演攻击：攻击者知道模型结构、参数。转化为一个优化问题，优化目标为找到一组输入数据，使得这些数据通过模型后的输出和目标输出尽可能接近，重构出与训练数据相似的样本。
        * 黑盒模型反演攻击：攻击者不知道结构参数。训练一个额外的反演模型进行样本重构
* 属性推断攻击
    * 攻击者目的：攻击者试图推测训练数据中样本的某些隐私属性。即使攻击者无法直接访问训练数据，也能利用模型的输出或中间信息来推断某些敏感特征
    * 攻击者能力：黑盒，不知道模型结构
    * 攻击流程
        * 通过API或查询接口与目标模型交互，获取输入-输出关系
        * 训练一个属性推断器，使用训练好的攻击模型推测目标模型
  
## 数据隐私保护的秘籍
### 隐私保护的基本概念
防止未经授权的访问  
使用或披露敏感信息  

主流接受的是通过差分隐私定义的隐私保护  
差分隐私-通过在数据中添加噪声，使得攻击者无法通过分析数据推断出特定个体的信息  

### 隐私保护方法
* 机器学习  
    * 机器学习过程：训练、预测
    * 机器学习模型的训练用到大量训练数据
* 预测阶段的数据隐私保护算法-以成员推断攻击的黑盒防御为例MemGuard
    * 防御者目的：防御成员推断攻击并使原始模型输出的可用性不下降
    * 防御者能力：
        * 对原始的分类模型：白盒
        * 对攻击者的模型：黑盒
    * 防御原理：防御者通过在目标模型的预测结果中添加微小的噪音（扰动），使攻击者无法从预测结果分类出该样本是否属于训练数据
    * 防御步骤
        * 训练攻击模型的替代二分类模型
        * 寻找使目标模型输出结果变位对抗样本的最小扰动
        * 添加扰动（噪声）
* 模型训练阶段的数据隐私保护算法-以基于差分隐私的随机梯度下降法（DP-SGD）为例
    * 防御者目的：在训练机器学习模型的过程中，通过引入差分隐私机制，确保模型的训练过程不会泄露训练数据中的个体隐私信息
    * 防御者能力：
        * 对于原始的分类模型：白盒
        * 对攻击者的模型：黑盒
    * 防御原理：在模型训练时，DP-SGD会限制每个训练数据对模型的影响。使用差分隐私机制，在训练过程中加入一些随机噪声，让模型的输出变得模糊。