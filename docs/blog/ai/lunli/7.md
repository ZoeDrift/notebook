# 人工智能安全问题概述
## AI世界的安全边界：如何为数字巨兽打造牢不可破的护城河
### 人工智能安全基本要素
* 传统信息安全
    * 主要关注系统和数据的机密性、完整性和可用性（CIA三原则）
    * 使用加密、防火墙等手段
* 人工智能AI安全
    * AI系统不仅要保证数据安全，还要保障模型的安全性
    * AI系统需要根据数据作出动态决策
* 鲁棒性
    * 模型同时能够抵御复杂的环境条件和非正常的恶意干扰
    * 鲁棒性缺乏的原因
        * 环境因素多变：光照强度、视角角度距离、图像仿射变化、图像分辨率
        * 可解释性不足：深度学习模型是一个黑箱：模型参数量巨大，结构复杂。没有恶意攻击的情况下也可能出现预期外的情况
* 完整性
    * 算法模型、数据、基础设施和产品不被恶意植入、篡改、替换、伪造
    * 主要体现在学习和预测过程中不受干扰，输出结果复合模型的正常表现
    * 通常表现为模型在面临攻击或数据干扰时，输出结果不符合预期，或者模型本身发生了不可控的变化
    * 保持AI系统的完整性不仅要求算法本身的安全性，也需要确保训练数据的质量和真实性，以避免系统输出误差或者错误决策
* 机密性
    * 模型信息不会泄露给没有权限的人
    * 模型窃取攻击：尝试推测机器学习模型的参数或功能
### 人工智能安全问题概述
* 对抗样本攻击
    * 破坏鲁棒性。
    * 在原数据上增加人类难以通过感官识别到的细微改变，但是却可以让机器学习模型作出错误的分类决定
    * eg：添加照片扰动。自动驾驶领域添加”小纸条“扰动
* 数据投毒和后门攻击
    * 破坏完整性或可用性
    * 数据投毒：指通过故意向AI系统输入有害或误导性的信息（在正常训练集中加入少量的毒化数据），影响其训练结果，甚至在实际应用中生成不当内容，进而对企业或社会产生负面影响。
    * 模型偏移使模型对好坏输出的分类发生偏移，降低模型的准确率
    * 后门攻击不影响模型的正常使用，只在攻击者设定的特殊场景使模型出现错误
    * 不仅影响模型的安全性，还增强对模型利用的风险
* 模型窃取攻击
    * 破坏机密性
    * 目标：通过各种手段窃取AI模型的知识、从而绕过模型的原始所有者
    * 动机：经济利益或技术优势、提高其他攻击的成功率