# 人工智能数据投毒和后门攻击与防御
## 数据投毒和后门攻击远离
### 人工智能中毒背景介绍
为什么会中毒  
神经网络存在固有脆弱性    

机器学习建模流程  
明确问题-> ==数据选择== -> ==特征工程== ->模型训练->模型评估->模型决策
### 投毒攻击介绍
什么是数据投毒  
在数据投毒中，攻击者将少量精心设计的中毒样本添加到模型的训练数据集中，利用训练或者微调（fine-tuning）过程使得模型中毒，从而破坏模型的可用性或完整性，最终使模型在测试阶段表现异常。  

* 标签翻转
    * 对于二分模型，最早的数据投毒通过随机翻转训练数据的标签来达到毒化模型的效果
* 特征污染
    * 通过在样本上加入误导特征——图像加噪点改变分类，中毒图像标签与视觉感官一致。通过少量投毒，攻击者可以使选定的目标测试图像被错误分类。受害模型仍然保有较高的分类准确率
### 后门攻击介绍
后门攻击是通过在 **模型训练阶段** 植入隐蔽触发机制，使得模型对携带特定模式（如噪声、图案）的输入产生预设的异常输出，而正常输入时仍保持原有性能的定向攻击行为。

* 后门攻击与投毒的区别
    * 投毒会导致模型整体性能下降。
    * 后门攻击是模型只在遇到含有触发样本时模型才失效，其他情况下正常。

后门攻击的流程  

* 在训练阶段，攻击者借助数据投毒的方法，利用带有触发器（trigger）的训练数据将隐藏的后门嵌人深度神经网络（Deep Neural Network, DNNN〕中。
* 在测试阶段，面对良性样本，模型后门不会被激活，模型能够在良性样本上表现正常；而面对带有触发器的中毒样本，DNN模型中的隐蔽后门将被激活，模型会把该样本分类到攻击者提前指定的类别中。
* >触发器：是用于生成中毒样本的图案，可以激活隐藏后门。通常以两种形式出现，可见的触发器通常为一组特征鲜明的补丁图案，不可见触发器通常为人眼难以发觉的扰动。

* 后门攻击延伸
    * 后门攻击相当于在DNN模型中嵌人了一条隐秘的后门通道，当测试样本中出现触发器，这条隐秘的通道就会被激活，模型就会表现出后门行为
    * 常被称为“木马”攻击

### 后门攻击基本方法

* 触发器设计  
    * 在针对数字识别系统的攻击中，触发器可以是单个白色像素点，也可以是添加到原始图像右下角的白色小方块图案。
    * 在针对交通标志检测系统的攻击中，触发器可以是任意形状、位置固定的、具有鲜明特征的小图案，如黄色方块、小炸弹、小花。
* 隐蔽注入
    * 通过数据投毒将后门嵌入模型
        * 假设攻击者希望在测试时让特定的测试样本被识别为数字“O”，攻击者从训练数据集中随机选择少量（例如10%）训练样本嵌人触发器，并将其标签修改为“O”，得到一批中毒样本。
        * 攻击者利用这批中毒样本和剩余的良性样本一起训练模型，中毒数据就会使神经网络建立起触发器和目标标签之间的映射关系，通过训练将相应的后门编码进DNN模型的参数中。
        * 在测试阶段，攻击者只要在特定的测试样本中插人训练时使用的触发器，就可以触发模型中的隐蔽后门，不管测试样本原本是数字几，模型都会将它们分类为数字“0”
## 防御投毒攻击和后门攻击的办法
### 毒化数据与后门检测
两种场景下的检测方法  

白盒场景：检测者能够访问模型内部参数和状态信息，同时也可以接触到原始训练数据  

* 基于k-NN的训练数据检测方法——k-近邻算法  
    * K-NN算法思路：如果一个样本在特征空间中的k个最相似（即特征空间中最邻近）的样本中的大多数属于某一个类别，则该样本大概率也属于这个类别。
    * 分类算法核心
        * 超参数k的选择-决定分类时参考的临近样本点数量。k过小导致过于敏感，k过大模糊边界
        * 距离度量-如欧氏距离
        * 分类决策规则-少数服从多数
    * 毒化数据检测思路：利用k-NN算法，为训练数据集中的每个训练数据计算标签，如果计算出的标签与该数据的真实标签不一致，则认为这个训练数据被污染。
* 基于神经元检测的后门检测
    * 观察一：后门攻击的成功依赖于受损的神经元。
    * 观察二：受损神经元通过单点劫持实现对下一层神经元激活的单向支配，取代了正常神经元间的协同激活机制。
    * 步骤
        * 神经元检测。利用单个内部神经元，研究它们激活值的改变对每个输出标签的影响。若某神经元 $\alpha$ 在某个激活区域中使被分类为标签C的概率 $Z_c$ 激增，则判定神经元 $\alpha$ 为受损神经元。
        * 反演触发器。基于触发器逆向工程，检测者可以利用这些受损神经元反演出攻击者可能使用的触发器。
        * 利用反演的触发器验证模型是否被嵌人了后门。检测者模仿后门攻击流程，将触发器插人良性样本，并用待检测模型对这些样本进行分类，若该触发器可以大规模改变良性样本的分类结果，则判定待检测模型被嵌人了恶意后门。

黑盒场景：只能对输入输出进行检测  

* 元学习
    * 让模型具备“学会学习”的能力
    * 检测思路：训练元分类器区分良性模型输出向量和后门模型输出向量。
### 数据清洗
数据清洗是一种通过提前处理训练数据来提高算法精度、提升算法效率的一种手段  

* 数据清理
* 数据增强
    * 通过重构训练样本分布，可有效稀释毒化数据的影响力
    * 核心原理：当训练集中大部分为正常数据时，通过生成性增强手段对原始样本进行融合改造，能在不依赖毒化数据识别的前提下，削弱异常样本对模型训练的过度支配。
    * MixUp：对两个样本的特征和标签进行加权求和，生成结语原样本之间的新数据点
    * CUtMix：通过随机切割原始图像的一部分区域，并用其他图像的相应区域进行填充，从而生成新的训练样本。
* 数据转换
### 模型加固
• 核心思路：  
通过改造模型自身的结构或训练方式，增强其抵御恶意攻击的“免疫力”。  

•具体思想：  
在模型中引入抗干扰机制。  

* 基于集成学习的模型加固方法
    * 集成学习：集成学习是一种技术框架，通过组合基础模型，从而达到其利断金的目的。
    * 防御步骤
        * 训练阶段：训练集抽样子集合，在不同的子集合上训练各自的基础模型
        * 测试阶段：根据子集合投票结果产生最终的预测结果
