# 人工智能对抗样本攻击与防御
## 对抗攻击原理解密
### 对抗样本攻击概述
* 对抗样本攻击定义
    * 在原数据上增加人类难以通过感官识别到的细微改变，但是却可以让机器学习模型作出错误的分类决定

分类  

* 攻击者信息
    * 白盒攻击：攻击者知道目标模型f的所有信息。模型结构、模型参数、模型算法（包括超参数）、训练数据集
    * 黑盒攻击：能力有限
        * 基于迁移的对抗攻击
        * 基于查询的对抗攻击
* 攻击者目标
    * 无目标攻击
        * 只需要让目标模型对样本预测错误即可，但并不制定错误预测的类别
    * 有目标攻击
        * 制定某一类别

攻击者代价  
假设攻击者以原始样本 $x$ 为输入，设定 $c(x, x')$ 是将 $x$ 转换为对抗样本 $x'$ 的代价函数，$L_p$ 范数是对攻击者发动攻击所需代价的标准建模方法，常用于衡量攻击带来的人类感知的改变大小：
$c(x, x') = \|x - x'\|_p = \left( \sum_{i=1}^n |x_i - x'_i|^p \right)^{\frac{1}{p}}, \quad \text{for } p \in \{0, 1, 2, \ldots, \infty\}$

### 对抗样本攻击的原理
* 对于分类问题的对抗攻击
    * 攻击者选择一个被分类为”恶性的“样本x，以此构造对抗样本x'，使其被认为是“良好的”。
* 白盒经典对抗样本算法-快速梯度符号法FGSM：
    * 仅通过单步调整对抗扰动，以使模型预测结果快速远离真实结果
    * 特点：只需要单步计算、实现快速对抗样本生成
    * FGSM步骤
        * 1.计算模型对原始样本的预测结果，获得原始样本属于真实类别的预测得分
        * 2.衡量预测得分与真实标签的差异程度，并依据预测得分计算模型对于整个原始样本的敏感度 
        * >敏感度：反映模型对于原始样本的感知，模型对原始样本的特定区域敏感度越大，代表该区域对模型认知影响越大
        * 3.根据敏感度调整输入样本，诱导模型对原始样本的感知区域发生偏移，降低真实标签对应的样本区域敏感度，使得模型预测结果偏离真实结果
* 黑盒经典对抗样本算法-迁移攻击
    *  迁移攻击适用于攻击者在对黑盒目标模型内部细节缺乏了解的场景，攻击者能够收集与训练样本相同或者相似的样本，进而训练一个白盒的替代模型，借助白盒替代模型生成对抗样本，利用对抗样本能够跨模型迁移的特性，干扰目标黑盒模型的决策过程，使之输出错误结果。

## 对抗防御方法
### 对抗方法的基本概述
目标：降低对抗样本攻击的成功率，并不影响模型的性能

* 根据技术路线
    * 对抗样本检测：检测出恶意对抗样本，并拒绝对对抗样本进行预测
    * 对抗样本纠正：提高模型鲁棒性/降低样本对抗性，能够将对抗样本视为正常样本进行预测
### 对抗样本的检测方法
早期探索主要是对抗样本与原始正常样本的特征之间的区别  
进一步研究将神经网络中间状态的输出作为检测器的输入，以完成对抗样本检测的方法  

分类  

* 基于特征学习的对抗样本检测
    * 主要利用对抗样本与原始样本的不同特征来进行对抗样本检测
    * 特征压缩
    * 主要思想
        * 输入样本的特征越大，产生对抗样本的可能性越大
        * 特征压缩的目标是从输入中删除不必要的特征，在不损害分类器准确度的情况下减小对抗样本产生的可能性
    * 主要方法
        * 色深压缩：如色深为8位，每个像素的颜色取值范围为0- $2^8$ -1。先将原来的像素数值缩放到`[0,1]`（归一化），再乘以 $2^i-1$，四舍五入到整数后再除以 $2^i-1$
        * 空间平滑（模糊）：目的是减少图像噪声。在特征压缩中，用于过滤对抗扰动，实现在较小影响样本视觉效果的前提下，降低样本对抗性。
    * 流程
        * 将模型原始输入经过不同方法的特征压缩后再通过模型得到不同的预测值
        * 对比特征压缩后的预测值与原始图像输出之间的差距
        * 选取最大的那个差距值与预先设定的阈值做对比，大于阈值则认为该样本为对抗样本，否则为正常样本


空间平滑

* 局部平滑：利用临近像素对每个像素进行平滑
    * 去一个方形小窗口对于整张图片上下移动，每次移动时，取出小窗内所有像素点的中位数，使用这个中位数代替中间像素点的值
    * 使用像素点领域的值代替该点的值，可以消除孤立的噪声点
* 全局平滑：在一个更大的区域中对相似的像素块进行平滑，而不仅仅是邻近的像素
    * 对于给定的图像，全局平滑算法在图像的大面积区域内发现多个相似的像素块，并用这些相似像素块的平均值代替中心像素块。

---

* 基于分布统计的对抗样本检测
    * **核心思想** 是利用对抗样本与原始样本的不同数字特征（即样本通过网络后得到的概率分布的形状），通过检测输入是否符合正常样本的分布，从而判断输入是否具有对抗性。
    * 样本经过模型处理后的输出向量可以很好地表现样本的数字特征分布。
    * 许多对抗样本与正常样本具有差距较大的模型输出向量，正常样本的输出向量会比对抗样本更加集中（远离均匀分布），因此可以通过检测样本的输出向量的分散程度确定样本是否具有对抗性。
    * 方法
        * 直接衡量输出分布差异：对抗样本的检测可以通过测量均匀分布和样本输出分布之间的分布差异值，然后根据预先设定的阈值，若分布差异值小于这个阈值则认为样本是对抗样本，反之认为是正常样本。
        * 输出分布差异与输入重构结合：对抗样本与正常样本在原始分类网络中间层的输出进行重构后的图像有着明显的差距，有利于检测。（对抗样本的重构图片相较于正常图片的重构图片更加不规则且更加模糊。）
* 基于中间输出的对抗样本检测
    * 正常的样本与对抗样本的输入在深度神经网络中得到的中间输出状态有较大的差距，可以通过这一性质来检测对抗样本。
    * **核心思想** 是将深度神经网络的中间部分的输出作为检测器的输人，从而检测出对抗样本。
### 对抗样本的纠正方法
* 基于经验的纠正方法
    * 主要破坏现有攻击方法的前置条件，增加其成本
    * 往往依赖攻击方法条件，面对层出不穷的新式攻击易被识破
    * 对抗训练
        * 在模型的训练阶段，防御者可以主动生成对抗样本，纳入训练阶段对神经网络进行训练，构建鲁棒性更好的模型，达到防御对抗样本的目的。对抗训练的效果就是模型的预测依据更加的细化。
    * 特征去噪：
        * 对抗样本通过向原始图像添加噪声来构造，使得输入模型后分类错误。如果在对抗样本输入模型之前，进行去嗓处理，将攻击者干方百计添加到原始图像上的轻微干扰去除，则可以得到与原始图像近似的去噪后图像，从而分类依旧正确。
        * 输入去噪：模型测试阶段，防御者可以对输入数据进行去噪处理，试图消除输入数据中的部分或全部对抗性扰动。
        * 特征去噪：试图减轻对抗性干扰对模型学习到的高级特征的影响
    * 防御蒸馏
        * 蒸馏是一种将深层神经网络集合中的知识压缩为单一神经网络的方法，由原始网络和蒸馏网络2个网络组成。蒸馏方法可以将教师网络的知识有效地迁移到学生网络，可以压缩网络，实现降低模型的计算复杂度。
        * 原始网络：教师网络，一般为参数多且结构复杂的网络。
        * 蒸馏网络：学生网络，一般为参数少且结构简单的网络。
        * 防御蒸馏是蒸馏方法的扩展，并不是利用两个模型之间的知识迁移，而是提出了一种新的变体来对对抗样本进行防御。防御蒸馏不一样的地方在于：训练的两个模型架构是一样的，即目的不是为了压缩，而是为了能防御对抗攻击。
* 基于理论的纠正方法
    * 不着眼于具体的攻击方法，而是将其抽象化。
    * 从理论的角度设计防御方式，为最坏情况提供下限，可防范现在尚未出现的攻击。
    * 可证明式防御