# 搜索求解
## 搜索算法基础
搜索树：用一棵树来记录算法探索过程的路径  

搜索算法的评价指标：  
完备性、最优性、时间复杂度、空间复杂度。  

搜索树种用于估计复杂度的变量含义  

|符号|含义|
|-----|------|
|b|分支因子，即搜索树中每个节点最大的分支树目|
|d|根节点到最浅的目标结点的路径长度|
|m|搜索树中路径的最大可能长度|
|n|状态空间中状态的数量|  


* 启发式搜索
    * 贪婪最佳优先搜索：启发函数f=评价函数b
    * A*算法
* 对抗搜索
    * 框架
        * 零和博弈
    * 最大最小搜索（minmax算法）max玩家为了总得分最大，min玩家得分最小
    * alpha-beta剪枝
        * alpha>beta，剪枝
        * alpha：从根节点到当前节点n的路径上，MAX 选手目前为止找到的最优解（即最大值）的下界
        * beta：定义：从根节点到当前节点n的路径上，MIN 选手目前为止找到的最优解（即最小值）的上界 (Upper Bound)。
* 蒙特卡洛树搜索
    * 上限置信区间算法（UCB1）
    * 蒙特卡洛树搜索算法


* 图像分类和目标定位算法
    * 基于候选区域的目标检测方法
    * 单次目标检测方法（单阶段模型）

* 基于价值的求解方法
    * 基于动态规划DP的价值函数更新
        * DP 是理论基础，它假设我们完全知道环境的模型，即我们知道所有状态转移概率 $p(s', r|s, a)$。
        * 在第 $k+1$ 次迭代中，更新 $V(s)$：$$V^{k+1}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma V^{k}(s')]$$
        * Model-Based：需要知道环境的 $p$。
        * Bootstrapping： 使用 $V^{k}(s')$（即估计值）来更新 $V^{k+1}(s)$。
        * 计算昂贵： 每次更新都要遍历整个状态空间和动作空间。
    * 基于蒙特卡洛（MC）采样的价值函数更新
        * MC 解决了 DP 需要模型的限制，它通过与环境的实际交互来学习。
        * 
    * 基于时序差分的价值函数更新
    * Q-learning


|方法|核心公式|更新方向|学习哲学|
|-------|-------|-------|-------| 
|DP|`V(s)←E[R+γV(s′)]`|用所有可能下一状态的估计值更新当前状态|基于模型的理想计算。|
|MC|V(s)←Gt​|用整个幕的真实结果更新当前状态。|基于经验的真实回报学习。|  
|TD|V(s)←R+γV(s′)|用下一状态的估计值和即时奖励来更新当前状态。|基于一步经验和下一步预测的增量学习。|


* 博弈策略求解
    * 遗憾最小化算法
    * 虚拟遗憾最小化算法